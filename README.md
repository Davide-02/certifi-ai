# CertiFi AI Pipeline

Pipeline modulare per l'estrazione di informazioni da documenti (fatture, diplomi, documenti di identitÃ , ecc.)

## ğŸ¯ Filosofia

**Non un'AI che fa tutto, ma una pipeline modulare** dove ogni componente Ã¨ sostituibile.

```
Upload â†’ Pre-processing â†’ Classification â†’ Text Extraction â†’ Information Extraction â†’ Normalization â†’ Validation
```

## ğŸ“ Struttura

```
certifi-ai/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py      # Normalizzazione testo
â”‚   â”œâ”€â”€ ocr.py               # Estrazione testo (PDF/immagini)
â”‚   â”œâ”€â”€ classifier.py        # Classificazione documenti (regole + LLM)
â”‚   â”œâ”€â”€ extractor.py         # Estrazione informazioni (regex + LLM)
â”‚   â”œâ”€â”€ validators.py        # Validazione dati estratti
â”‚   â”œâ”€â”€ orchestrator.py      # Orchestratore principale
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ base.py
â”‚       â”œâ”€â”€ invoice.py
â”‚       â”œâ”€â”€ diploma.py
â”‚       â””â”€â”€ id_document.py
â”œâ”€â”€ main.py                  # Esempio di utilizzo
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Installazione

1. **Installa dipendenze:**
```bash
pip install -r requirements.txt
```

2. **Installa Tesseract OCR:**
```bash
# macOS
brew install tesseract tesseract-lang

# Ubuntu/Debian
sudo apt-get install tesseract-ocr tesseract-ocr-ita

# Windows
# Scarica da: https://github.com/UB-Mannheim/tesseract/wiki
```

3. **Configura variabili d'ambiente (opzionale, per LLM):**
```bash
# Copia il file di esempio
cp .env.example .env

# Modifica .env con le tue chiavi API
# oppure esporta direttamente:
export OPENAI_API_KEY="your-key-here"
export USE_LLM="true"  # Per abilitare LLM
```

4. **Testa l'installazione:**
```bash
python test_pipeline.py
```

## ğŸ’» Utilizzo

### Esempio base

```python
from pipeline.orchestrator import DocumentPipeline

# Inizializza pipeline
pipeline = DocumentPipeline(use_llm=False)

# Processa documento
result = pipeline.process("path/to/document.pdf")

# Risultato
print(result['document_type'])  # 'invoice', 'diploma', 'id', etc.
print(result['data'])           # Schema Pydantic con dati estratti
print(result['validation'])     # Risultato validazione
```

### Con LLM (opzionale)

```python
pipeline = DocumentPipeline(use_llm=True, llm_provider="openai")
result = pipeline.process("document.pdf")
```

## ğŸ“‹ Tipi di Documenti Supportati

### 1. Fatture (Invoice)
- Numero fattura
- Data
- Venditore/Cliente
- Importi (totale, IVA, netto)
- Partita IVA

### 2. Diplomi (Diploma)
- Nome studente
- UniversitÃ 
- Tipo laurea
- CFU
- Data conseguimento
- Voto finale

### 3. Documenti IdentitÃ  (ID)
- Nome/Cognome
- Data di nascita
- Codice fiscale
- Numero documento
- Indirizzo

## ğŸ”§ Componenti

### 1. Text Extraction (`ocr.py`)
- PDF con testo: `pdfplumber`, `PyMuPDF`
- PDF scannerizzati: OCR con `pytesseract`
- Immagini: OCR con preprocessing

### 2. Classification (`classifier.py`)
- **Livello 1**: Regole + keyword matching (80% dei casi)
- **Livello 2**: LLM fallback (quando incerto)

### 3. Information Extraction (`extractor.py`)
- **Livello 1**: Regex patterns (veloce, robusto)
- **Livello 2**: LLM extraction (quando regex fallisce)

### 4. Validation (`validators.py`)
- Validazione campi obbligatori
- Controllo coerenza dati
- Confidence scoring

## ğŸ¯ Strategia di Sviluppo

### âœ… Fase 1 (Settimana 1)
- [x] OCR + PDF extraction
- [x] Dump testo pulito

### âœ… Fase 2 (Settimana 2)
- [x] Estrazione rule-based
- [x] JSON schema (Pydantic)

### ğŸ”„ Fase 3 (Settimana 3)
- [ ] Fallback LLM
- [ ] Validazione output avanzata

### ğŸ“… Fase 4 (Settimana 4)
- [ ] Confidence score migliorato
- [ ] Integrazione CertiFi on-chain

## ğŸ” Hash Canonico

Per CertiFi, ogni documento genera un hash canonico basato sui dati strutturati:

```python
result['metadata']['canonical_hash']  # SHA256 del JSON canonico
```

Questo hash puÃ² essere usato per:
- Verifica on-chain
- Deduplicazione
- IntegritÃ  dati

## âš ï¸ Note Importanti

1. **Non partire da ML avanzato**: Inizia con regole, poi aggiungi LLM
2. **Un tipo documento alla volta**: Diventa eccellente su un dominio prima di generalizzare
3. **Pipeline > Modello**: Ogni blocco Ã¨ sostituibile (leva tecnica)
4. **Validazione critica**: Per CertiFi, i dati devono essere validati prima dell'on-chain

## ğŸ› ï¸ Estendere la Pipeline

### Aggiungere un nuovo tipo documento

1. Crea schema in `schemas/`:
```python
class NewDocumentSchema(BaseDocumentSchema):
    document_type: str = Field(default="new_type", const=True)
    field1: Optional[str] = None
    # ...
```

2. Aggiungi pattern in `classifier.py`:
```python
'new_type': {
    'keywords': ['keyword1', 'keyword2'],
    'patterns': [r'pattern1', r'pattern2'],
    'min_matches': 2
}
```

3. Aggiungi estrazione in `extractor.py`:
```python
def _extract_new_type(self, text: str) -> NewDocumentSchema:
    # Regex patterns
    # ...
```

## ğŸ“ Esempi

Vedi `main.py` per un esempio completo di utilizzo.

## ğŸ”— Integrazione CertiFi

La pipeline Ã¨ progettata per integrarsi con CertiFi:

1. **Estrazione** â†’ Dati strutturati
2. **Validazione** â†’ Controllo qualitÃ 
3. **Hash canonico** â†’ Per on-chain storage
4. **Schema standardizzato** â†’ Per smart contracts

## ğŸ“„ Licenza

[Da definire]
